{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, mean,expr, avg, stddev\n",
    "from pyspark.sql.functions import lag, coalesce, lit\n",
    "from pyspark.sql.functions import corr\n",
    "from pyspark.sql.functions import to_date, date_format\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Investiagtion and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|2018-02-05|     262.0|267.899994|250.029999|254.259995|254.259995|11896100|\n",
      "|2018-02-06|247.699997|266.700012|     245.0|265.720001|265.720001|12595800|\n",
      "|2018-02-07|266.579987|272.450012|264.329987|264.559998|264.559998| 8981500|\n",
      "|2018-02-08|267.079987|267.619995|     250.0|250.100006|250.100006| 9306700|\n",
      "|2018-02-09|253.850006|255.800003|236.110001|249.470001|249.470001|16906900|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Netflix Stock Price Forecasting\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# Load the data\n",
    "df = spark.read.csv(\"C:/Users/ADMIN/Desktop/Scalable and Distributed Computing/Stock-Price-Forecasting-System-Using-Apache-Spark/data/raw/NFLX.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n",
      "Rows: 1009, Columns: 7\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "print(f\"Rows: {df.count()}, Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|              Open|              High|               Low|             Close|         Adj Close|           Volume|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|              1009|              1009|              1009|              1009|              1009|             1009|\n",
      "|   mean|419.05967286223984|425.32070308027744|412.37404380178384| 419.0007329207132| 419.0007329207132| 7570685.03468781|\n",
      "| stddev|108.53753170401458|109.26295957119454|107.55586739006031|108.28999877034995|108.28999877034995|5465535.225689975|\n",
      "|    min|        233.919998|        250.649994|        231.229996|        233.880005|        233.880005|          1144000|\n",
      "|    max|        692.349976|         700.98999|        686.090027|        691.690002|        691.690002|         58904300|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+---------+------+\n",
      "|Date|Open|High|Low|Close|Adj Close|Volume|\n",
      "+----+----+----+---+-----+---------+------+\n",
      "|   0|   0|   0|  0|    0|        0|     0|\n",
      "+----+----+----+---+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values and remove them\n",
    "missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_values.show()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|2018-02-05|     262.0|267.899994|250.029999|254.259995|254.259995|11896100|\n",
      "|2018-02-06|247.699997|266.700012|     245.0|265.720001|265.720001|12595800|\n",
      "|2018-02-07|266.579987|272.450012|264.329987|264.559998|264.559998| 8981500|\n",
      "|2018-02-08|267.079987|267.619995|     250.0|250.100006|250.100006| 9306700|\n",
      "|2018-02-09|253.850006|255.800003|236.110001|249.470001|249.470001|16906900|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the Date column to Timstamp\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"timestamp\"))\n",
    "# Only show \"yyyy-MM-dd\" in the Date column\n",
    "df = df.withColumn(\"Date\", date_format(col(\"Date\"),\"yyyy-MM-dd\"))\n",
    "\n",
    "df = df.orderBy(\"Date\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set row count: 807\n",
      "Testing set row count: 202\n"
     ]
    }
   ],
   "source": [
    "# Calculate split index\n",
    "split_index = int(df.count() * 0.8)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train = df.limit(split_index)  # Take the first 80% of rows\n",
    "test = df.subtract(train)      # Subtract the training set from the original DataFrame to get the test set\n",
    "test_copy = test.select(\"*\") \n",
    "# Display row counts of the resulting DataFrames to verify the split\n",
    "print(f\"Training set row count: {train.count()}\")\n",
    "print(f\"Testing set row count: {test.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pandas = test.toPandas()\n",
    "train_pandas = train.toPandas()\n",
    "\n",
    "\n",
    "# Convert the Date column to datetime format for accurate sorting\n",
    "test_pandas[\"Date\"] = pd.to_datetime(test_pandas[\"Date\"])\n",
    "train_pandas[\"Date\"] = pd.to_datetime(train_pandas[\"Date\"])\n",
    "\n",
    "# Sort the Pandas DataFrame by the Date column\n",
    "test_pandas = test_pandas.sort_values(by=\"Date\")\n",
    "train_pandas = train_pandas.sort_values(by=\"Date\")\n",
    "\n",
    "test_pandas.to_csv(\"../data/processed/test.csv\", index=False)\n",
    "train_pandas.to_csv(\"../data/processed/train.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
