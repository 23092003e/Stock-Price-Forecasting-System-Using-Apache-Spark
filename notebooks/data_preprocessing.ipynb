{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, mean,expr, avg, stddev\n",
    "from pyspark.sql.functions import lag, coalesce, lit\n",
    "from pyspark.sql.functions import corr\n",
    "from pyspark.sql.functions import to_date, date_format\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Investiagtion and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|2018-02-05|     262.0|267.899994|250.029999|254.259995|254.259995|11896100|\n",
      "|2018-02-06|247.699997|266.700012|     245.0|265.720001|265.720001|12595800|\n",
      "|2018-02-07|266.579987|272.450012|264.329987|264.559998|264.559998| 8981500|\n",
      "|2018-02-08|267.079987|267.619995|     250.0|250.100006|250.100006| 9306700|\n",
      "|2018-02-09|253.850006|255.800003|236.110001|249.470001|249.470001|16906900|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Netflix Stock Price Forecasting\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# Load the data\n",
    "df = spark.read.csv(\"C:/Users/ADMIN/Desktop/Scalable and Distributed Computing/Stock-Price-Forecasting-System-Using-Apache-Spark/data/NFLX.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n",
      "Rows: 1009, Columns: 7\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "print(f\"Rows: {df.count()}, Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|              Open|              High|               Low|             Close|         Adj Close|           Volume|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|              1009|              1009|              1009|              1009|              1009|             1009|\n",
      "|   mean|419.05967286223984|425.32070308027744|412.37404380178384| 419.0007329207132| 419.0007329207132| 7570685.03468781|\n",
      "| stddev|108.53753170401458|109.26295957119454|107.55586739006031|108.28999877034995|108.28999877034995|5465535.225689975|\n",
      "|    min|        233.919998|        250.649994|        231.229996|        233.880005|        233.880005|          1144000|\n",
      "|    max|        692.349976|         700.98999|        686.090027|        691.690002|        691.690002|         58904300|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+---------+------+\n",
      "|Date|Open|High|Low|Close|Adj Close|Volume|\n",
      "+----+----+----+---+-----+---------+------+\n",
      "|   0|   0|   0|  0|    0|        0|     0|\n",
      "+----+----+----+---+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values and remove them\n",
    "missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_values.show()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|2018-02-05|     262.0|267.899994|250.029999|254.259995|254.259995|11896100|\n",
      "|2018-02-06|247.699997|266.700012|     245.0|265.720001|265.720001|12595800|\n",
      "|2018-02-07|266.579987|272.450012|264.329987|264.559998|264.559998| 8981500|\n",
      "|2018-02-08|267.079987|267.619995|     250.0|250.100006|250.100006| 9306700|\n",
      "|2018-02-09|253.850006|255.800003|236.110001|249.470001|249.470001|16906900|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the Date column to Timstamp\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"timestamp\"))\n",
    "# Only show \"yyyy-MM-dd\" in the Date column\n",
    "df = df.withColumn(\"Date\", date_format(col(\"Date\"),\"yyyy-MM-dd\"))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
