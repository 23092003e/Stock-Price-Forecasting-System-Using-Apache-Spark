{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import datetime\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, lit, date_add, explode\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|2018-02-05|     262.0|267.899994|250.029999|254.259995|254.259995|11896100|\n",
      "|2018-02-06|247.699997|266.700012|     245.0|265.720001|265.720001|12595800|\n",
      "|2018-02-07|266.579987|272.450012|264.329987|264.559998|264.559998| 8981500|\n",
      "|2018-02-08|267.079987|267.619995|     250.0|250.100006|250.100006| 9306700|\n",
      "|2018-02-09|253.850006|255.800003|236.110001|249.470001|249.470001|16906900|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Netflix Stock Price Forecasting\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# Load the data\n",
    "df = spark.read.csv(\"../data/raw/NFLX.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data intro train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  805\n",
      "Number of testing samples:  204\n"
     ]
    }
   ],
   "source": [
    "# Get the minimum and maximum dates in the dataset\n",
    "min_date = df.select(\"Date\").agg({\"Date\": \"min\"}).collect()[0][0]\n",
    "max_date = df.select(\"Date\").agg({\"Date\": \"max\"}).collect()[0][0]\n",
    "\n",
    "# Calculate the time delta between the first and last date\n",
    "delta = max_date - min_date\n",
    "\n",
    "# Define a cutoff date: use the first 80% of the time period for training\n",
    "cutoff_date = min_date + datetime.timedelta(days=int(delta.days * 0.8))\n",
    "\n",
    "# Split the dataset:\n",
    "# - train_df: data with Date less than cutoff_date (older data)\n",
    "# - test_df: data with Date on or after cutoff_date (newer data)\n",
    "train_df = df.filter(col(\"Date\") < cutoff_date)\n",
    "test_df = df.filter(col(\"Date\") >= cutoff_date)\n",
    "\n",
    "# Print out the counts in each set\n",
    "print(\"Number of training samples: \", train_df.count())\n",
    "print(\"Number of testing samples: \", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|2018-02-05|     262.0|267.899994|250.029999|254.259995|254.259995|11896100|\n",
      "|2018-02-06|247.699997|266.700012|     245.0|265.720001|265.720001|12595800|\n",
      "|2018-02-07|266.579987|272.450012|264.329987|264.559998|264.559998| 8981500|\n",
      "|2018-02-08|267.079987|267.619995|     250.0|250.100006|250.100006| 9306700|\n",
      "|2018-02-09|253.850006|255.800003|236.110001|249.470001|249.470001|16906900|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-based Split: Splitting by time helps prevent “look-ahead bias” where future information inadvertently influences the model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# List of feature columns from the Feature Engineering step\n",
    "feature_columns = [\"Open\", \"High\", \"Low\", \"Volume\"]\n",
    "\n",
    "# Create a VectorAssembler to combine these columns into one vector column \"scaledFeatures\"\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns, \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "train_df = assembler.transform(train_df).select(\"features\", col(\"Close\").alias(\"label\"))\n",
    "test_df = assembler.transform(test_df).select(\"features\", col(\"Close\").alias(\"label\"))\n",
    "\n",
    "# Create Scaler to normalize vector\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaledFeatures\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Create LR model use scaledFeatures as input\n",
    "linear_regressor = LinearRegression(\n",
    "    featuresCol = \"scaledFeatures\",\n",
    "    labelCol = \"label\"\n",
    ")\n",
    "\n",
    "#Build the pipeline that includes scaling and regression\n",
    "pipeline = Pipeline(stages=[scaler, linear_regressor])\n",
    "\n",
    "#Configure hyperparameter tuning with grid search\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(linear_regressor.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(linear_regressor.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(linear_regressor.maxIter, [50, 100, 200]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Params:\n",
      "  Regularization Param (regParam): 0.01\n",
      "  ElasticNet Param (elasticNetParam): 0.0\n",
      "  Maximum Iterations (maxIter): 50\n"
     ]
    }
   ],
   "source": [
    "#Set up cross-validation\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "crossval = CrossValidator(\n",
    "    estimator = pipeline,\n",
    "    estimatorParamMaps = paramGrid,\n",
    "    evaluator = evaluator,\n",
    "    numFolds = 5  # Number of folds for cross-validation\n",
    ")\n",
    "\n",
    "# Fit the cross-validator to find the best model\n",
    "cv_model = crossval.fit(train_df)\n",
    "\n",
    "# Get the best model (which is a fitted pipeline)\n",
    "best_pipeline_model = cv_model.bestModel\n",
    "best_model = best_pipeline_model.stages[-1]  # Extract the LinearRegression model\n",
    "\n",
    "print(\"Best Model Params:\")\n",
    "print(\"  Regularization Param (regParam):\", best_model.getRegParam())\n",
    "print(\"  ElasticNet Param (elasticNetParam):\", best_model.getElasticNetParam())\n",
    "print(\"  Maximum Iterations (maxIter):\", best_model.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 17.161\n",
      "RMSE: 4.143\n",
      "MAE: 3.07\n",
      "R2 Score: 0.997\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set using the pipeline\n",
    "predictions = best_pipeline_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model \n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(\"MSE:\", round(mse, 3))\n",
    "print(\"RMSE:\", round(rmse, 3))\n",
    "print(\"MAE:\", round(mae, 3))\n",
    "print(\"R2 Score:\", round(r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
